{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Product Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Building a system where users can search for products using text or images. Will be using CLIP for the image/text embeddings, then store these embeddings in FAISS. Rank the embeddings by cosine similarity and create a front-facing UI for users to search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following concepts will be used:\n",
    "<br>\n",
    "1. Multimodal embeddings\n",
    "2. Shared embedding space (text & image will be embedded into the same latent space)\n",
    "3. Image processing\n",
    "4. Cosine similarity\n",
    "5. Vector database\n",
    "6. Retrieval & ranking\n",
    "7. UI\n",
    "<br>\n",
    "\n",
    "\n",
    "| Phase                            | Tasks                                                         | Est. Time |\n",
    "| -------------------------------- | ------------------------------------------------------------- | --------- |\n",
    "| **1. Research & Setup**          | Understand CLIP, FAISS, define scope, gather dataset          | 3–4 hrs   |\n",
    "| **2. Embedding Pipeline**        | Encode product images + text descriptions using CLIP/OpenCLIP | 3–5 hrs   |\n",
    "| **3. Indexing + Search**         | Use FAISS/LanceDB to build index, implement similarity search | 3–4 hrs   |\n",
    "| **4. Query System**              | Add text + image search input, return top-k products          | 2–3 hrs   |\n",
    "| **5. Frontend / UI (optional)**  | Build simple UI with Streamlit/Gradio to demo system          | 2–4 hrs   |\n",
    "| **6. Polishing + Writeup**       | Refactor code, write README, evaluate search quality          | 2–3 hrs   |\n",
    "| **7. Bonus Features (optional)** | Personalization, hybrid scoring, caching, dockerize           | 2–4 hrs   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP\n",
    "\n",
    "Foundation for most multi-modal search systems. CLIP embeds images and texts into a shared latent space and learns to push embeddings closer in \"meaning\" together. \n",
    "\n",
    "Overview: https://www.youtube.com/watch?v=KcSXcpluDe4\n",
    "\n",
    "Paper: https://arxiv.org/abs/2103.00020\n",
    "\n",
    "Paper explanation: https://www.youtube.com/watch?v=T9XSU0pKX2E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular File Hierarchy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultimodal-product-search/\\n│\\n├── data/                   # Raw and processed product data + images\\n│   ├── products.csv        # Your product catalog (name, category, description, image_path)\\n│   └── images/             # Product images\\n│\\n├── notebooks/              # For EDA or prototype testing\\n│   └── explore_clip.ipynb\\n│\\n├── src/                    # Core source code\\n│   ├── __init__.py\\n│   ├── config.py           # Paths, hyperparams\\n│   ├── data_loader.py      # Load product metadata + images\\n│   ├── embedder.py         # Load CLIP and generate embeddings\\n│   ├── indexer.py          # Build and query FAISS index\\n│   ├── search.py           # Combined image/text search logic\\n│   └── utils.py            # Shared functions (normalization, logging, etc.)\\n│\\n├── app/                    # UI or API interface (optional)\\n│   ├── streamlit_app.py    # If using Streamlit\\n│   └── api.py              # If building FastAPI/Flask backend\\n│\\n├── tests/                  # Unit tests\\n│   └── test_embedding.py\\n│\\n├── requirements.txt        # Package dependencies\\n├── README.md               # Project overview\\n└── run.py                  # Entry point script (e.g., build index, search interactively)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "multimodal-product-search/\n",
    "│\n",
    "├── data/                   # Raw and processed product data + images\n",
    "│   ├── products.csv        # Your product catalog (name, category, description, image_path)\n",
    "│   └── images/             # Product images\n",
    "│\n",
    "├── notebooks/              # For EDA or prototype testing\n",
    "│   └── explore_clip.ipynb\n",
    "│\n",
    "├── src/                    # Core source code\n",
    "│   ├── __init__.py\n",
    "│   ├── config.py           # Paths, hyperparams\n",
    "│   ├── data_loader.py      # Load product metadata + images\n",
    "│   ├── embedder.py         # Load CLIP and generate embeddings\n",
    "│   ├── indexer.py          # Build and query FAISS index\n",
    "│   ├── search.py           # Combined image/text search logic\n",
    "│   └── utils.py            # Shared functions (normalization, logging, etc.)\n",
    "│\n",
    "├── app/                    # UI or API interface (optional)\n",
    "│   ├── streamlit_app.py    # If using Streamlit\n",
    "│   └── api.py              # If building FastAPI/Flask backend\n",
    "│\n",
    "├── tests/                  # Unit tests\n",
    "│   └── test_embedding.py\n",
    "│\n",
    "├── requirements.txt        # Package dependencies\n",
    "├── README.md               # Project overview\n",
    "└── run.py                  # Entry point script (e.g., build index, search interactively)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Workflow\n",
    "\n",
    "Step 1: Data\n",
    "Place product metadata (name, description, image path) in data/products.csv\n",
    "\n",
    "        Place all product images in data/images/\n",
    "\n",
    "Step 2: Embedding\n",
    "embedder.py: Load CLIP, encode both:\n",
    "\n",
    "                Images (via encode_image)\n",
    "\n",
    "                Descriptions (via encode_text)\n",
    "\n",
    "Step 3: Indexing\n",
    "indexer.py: Store embeddings in FAISS or Weaviate\n",
    "\n",
    "                Save indexes for later use\n",
    "\n",
    "Step 4: Search Logic\n",
    "search.py: Given a text or image query, return top-k similar products\n",
    "\n",
    "                Can combine modalities (text + image fusion later)\n",
    "\n",
    "Step 5: Interactive App (Optional)\n",
    "Add streamlit_app.py to let users:\n",
    "\n",
    "                Upload image or enter text\n",
    "\n",
    "                View top-k product matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
